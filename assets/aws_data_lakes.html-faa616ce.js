import{_ as i,Y as o,Z as r,a0 as e,a1 as a,a2 as n,a4 as s,E as l}from"./framework-957baa9a.js";const c={},d=s('<h1 id="aws-data-lakes" tabindex="-1"><a class="header-anchor" href="#aws-data-lakes" aria-hidden="true">#</a> AWS Data Lakes</h1><p>在集中的仓库中存放任何规模的结构化和非结构化数据, 运行不同类型的分析来引导决策</p><h2 id="why-data-lakes" tabindex="-1"><a class="header-anchor" href="#why-data-lakes" aria-hidden="true">#</a> Why Data Lakes</h2><p>从数据中成功获取价值的组织, 将会超越他的同行.</p><p>In a few sentences, explain why is Amazon Simple Storage Service (Amazon S3) is the most appropriate AWS service to use for the data storage layer of data lakes on AWS. Review your peers’ answers and compare them with your answer to deepen your understanding.</p><ul><li>cheap(Amazon S3, Amazon S3 Glacier), You pay only for the space you use.</li><li>flexible and reliable storage</li><li>It could be used to connect to multiple sources.</li></ul><p>Knowledge Check: (14)222</p><p>quiz: 223(23)1 14</p><h2 id="aws-data-related-services" tabindex="-1"><a class="header-anchor" href="#aws-data-related-services" aria-hidden="true">#</a> AWS Data Related Services</h2><h3 id="aws-services-used-for-data-storage" tabindex="-1"><a class="header-anchor" href="#aws-services-used-for-data-storage" aria-hidden="true">#</a> AWS Services Used For Data Storage</h3><p>Amazon S3</p><ul><li>scalable, a service where the underlying infrastructure hosting the service is managed for you</li><li>provide 99.99999999% durablity</li><li>made accessible for tools and services</li><li>different cost tiers bring down the cost(s3 standard, s3 Glacier Deep Archive)</li></ul><p>AWS Glue</p><ul><li><code>AWS Glue Data Catelog</code> as the central metadata repository, a drop in replacement for Apache Hive Metastore</li><li><code>Glue crawler</code> is triggered to sort through your data in s3 and calls classifier logic to infer the schema, format, and data type.</li></ul><h3 id="aws-services-used-for-data-movement" tabindex="-1"><a class="header-anchor" href="#aws-services-used-for-data-movement" aria-hidden="true">#</a> AWS Services Used For Data Movement</h3>',15),p={href:"https://aws.amazon.com/kinesis/",target:"_blank",rel:"noopener noreferrer"},u={href:"https://aws.amazon.com/api-gateway/",target:"_blank",rel:"noopener noreferrer"},h=s(`<p>Data Exchange for data ingestion from SAS applications.</p><p>AppFlow for data ingestion from SAS applications, Data Exchange, Registry of Open Data for public datasets.</p><h3 id="aws-services-for-data-processing" tabindex="-1"><a class="header-anchor" href="#aws-services-for-data-processing" aria-hidden="true">#</a> AWS Services For Data Processing</h3><p><a href=".">Amazon EMR</a> is a managed cluster platform, manages the Hadhoop cluster. <a href=".">AWS Glue Job</a>, instead of creating a cluster to run your processing login on, you instead porvide a set of configurations. A job is composed of data sources, data targets, which come from the tables created in the AWS Glue Data Catelog, also has a transformation script as well as other customizations that you provide.</p><h3 id="aws-services-for-analytics" tabindex="-1"><a class="header-anchor" href="#aws-services-for-analytics" aria-hidden="true">#</a> AWS Services for Analytics</h3><p><code>Athena and Redshift</code> are great for analyzing data that has already been stored in S3.</p><pre><code>- [Amazon Athena](.) is a serverless service, run a SQL query against data which located in s3.
- Athena integrates with AWS Glue as it natively supports querying data sets and data sources that are registered with the AWS Glue Data Catelog.
- [Amazon Redshift](.) integrates directly S3, the data warehousing service.
</code></pre><p><code>AWS Open Search</code>(AWS Elastics Search)</p><p><code>Kinesis Analytics</code></p><h3 id="aws-lake-formation" tabindex="-1"><a class="header-anchor" href="#aws-lake-formation" aria-hidden="true">#</a> AWS Lake Formation</h3><ul><li>blueprint is a templat, take the data source, data target, and schedule.</li><li>workflow consist of AWS Glue crawlers, jobs, triggers</li></ul><p><img src="https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/K_38V1grQmy9_FdYK1JsIQ_86b7ae2c11b447a9bc701d207dc1a0da_Picture1-1-.png?expiry=1682467200000&amp;hmac=A4dlAVWsqlDupuG6UlEUFZwz9snU5_t6UPE0T0PjB2k" alt="how data is loaded and secured in Lake Formation"></p><h2 id="exercise1" tabindex="-1"><a class="header-anchor" href="#exercise1" aria-hidden="true">#</a> Exercise1</h2><p>Related Services: <code>IAM</code>, <code>API Gateway</code>, <code>AWS Lambda</code>, <code>Amazon OpenSearch Service</code>, <code>AWS S3</code></p>`,14),m={start:"0"},g=s("<li><p>Create role.</p><ul><li>Use case: Lambda</li><li>Permissions policies: AmazonS3FullAccess, AmazonESFullAccess</li></ul></li><li><p>Creating an Amazon OpenSearch Service cluster</p><ul><li>Deployment type: Development and testing</li><li>Data nodes &gt; Instance type: t3.small.search</li><li>Network: Public access</li><li>Fine-grained access control: Deselect this setting</li><li>Access policy &gt; Domain access policy <ul><li>Configure domain level access policy(Use IPv4 address to restrict access)</li></ul></li></ul></li><li><p>Creating an Amazon S3 bucket</p><ul><li><code>datalakes-week2-10086Info</code></li></ul></li><li><p>Creating the AWS Lambda function</p><ul><li>create function: <ul><li>Author from scratch</li><li>Function name: upload-data</li><li>Runtime: Select the latest supported version of Python</li><li>Permissions: Existing role: data-lake-week-2</li></ul></li><li>upload_code <ul><li>Code, Upload zip</li></ul></li><li>change handler <ul><li>Runtime settings &gt; Edit &gt; Handler &gt; lambda.handler</li></ul></li><li>Configuration Environment variables <ul><li>S3_BUCKET, datalakes-week2-10086</li><li>ES_DOMAIN_URL, https://search-water-temp-domain-7c5mt6asie2g5za7g4axxi6cii.us-west-2.es.amazonaws.com</li></ul></li></ul></li>",4),v=e("p",null,"Modifying the S3 bucket policy and OpenSearch Service cluster for Lambda access",-1),f={href:"https://awspolicygen.s3.amazonaws.com/policygen.html",target:"_blank",rel:"noopener noreferrer"},k=s(`<div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token punctuation">{</span>
    <span class="token property">&quot;Id&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Policy1682412146898&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;Version&quot;</span><span class="token operator">:</span> <span class="token string">&quot;2012-10-17&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;Statement&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{</span>
            <span class="token property">&quot;Sid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Stmt1682412064119&quot;</span><span class="token punctuation">,</span>
            <span class="token property">&quot;Action&quot;</span><span class="token operator">:</span> <span class="token string">&quot;s3:*&quot;</span><span class="token punctuation">,</span>
            <span class="token property">&quot;Effect&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Allow&quot;</span><span class="token punctuation">,</span>
            <span class="token property">&quot;Resource&quot;</span><span class="token operator">:</span> <span class="token string">&quot;arn:aws:s3:::dalakes-k---2-10086&quot;</span><span class="token punctuation">,</span>
            <span class="token property">&quot;Principal&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
                <span class="token property">&quot;AWS&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
                <span class="token string">&quot;arn:aws:iam::07865434:role/data-law---eek-2&quot;</span>
                <span class="token punctuation">]</span>
            <span class="token punctuation">}</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),b=s(`<li><p>Allows the Lambda function to upload the temperature files to your bucket.</p><ul><li><p>lambda function roles, data-lake-week-2</p></li><li><p>s3 policy:</p><div class="language-json line-numbers-mode" data-ext="json"><pre class="language-json"><code><span class="token property">&quot;Principal&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
    <span class="token property">&quot;AWS&quot;</span><span class="token operator">:</span> <span class="token string">&quot;arn:aws:iam::xxxxxxxxxxxx:role/data-lake-week-2&quot;</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></li><li><p>Allows the Lambda function to access OpenSearch.</p><ul><li>edit Security configuration</li></ul></li>`,2),S=e("li",null,[e("p",null,"Modifying the Lambda function and creating an API Gateway endpoint"),e("ul",null,[e("li",null,"Create a REST API, API Gateway > REST API > Build > rest api > creat api"),e("li",null,[a("add method "),e("ul",null,[e("li",null,"method: POST"),e("li",null,"Integration type setting at Lambda Function > upload-data")])])])],-1),y=e("li",null,[e("p",null,"Test")],-1),w=s(`<h3 id="quiz" tabindex="-1"><a class="header-anchor" href="#quiz" aria-hidden="true">#</a> Quiz</h3><p>1-5: 241444 6-9: 313(14)</p><h2 id="ingesting-the-rivers" tabindex="-1"><a class="header-anchor" href="#ingesting-the-rivers" aria-hidden="true">#</a> ingesting the Rivers</h2><h3 id="right-tool-for-the-job" tabindex="-1"><a class="header-anchor" href="#right-tool-for-the-job" aria-hidden="true">#</a> Right Tool for the job</h3><ol><li><p>Scenario 1:</p><ul><li>Auto-generated data from IoT devices or server logs, often come via streaming and are typically unstructured.</li><li>Suitable to ingest with <code>Amazon Kinesis</code>, store in <code>Amazon S3</code>, catalog with <code>AWS Glue</code>, process with <code>Lambda</code>, and query with <code>Amazon Athena</code></li></ul></li><li><p>Scenario 2:</p><ul><li>Operational data such as inventory and sales, expense reports and other inputs. Those likely come in batches and are usually consumed by people who want to visualize graphs and have access to statistics.</li><li>Suitable for being ingested with <code>API Gateway</code>, stored in <code>Amazon S3</code>, cataloged with <code>Glue</code>, transported to <code>Amazon Elasticsearch Service</code>(<code>AWS Open Search</code>), and visualized with <code>Kibana</code>.</li></ul></li><li><p>Scenario 3:</p><ul><li>Human generated data such as social media feeds, contact forms, call center audio, emails, et cetera. For these, you may think about access patterns needed by data analysis services.</li><li>Suitable to ingest that data with <code>S3 SFTP</code> or <code>AppFlow</code>, store it in <code>S3</code>, catalog with <code>Glue</code>, and use a service like <code>Amazon Comprehend</code>, which is a <code>natural language processing service</code> that uses machine learning to find insights in text, such as sentimental analysis.</li></ul></li></ol><h3 id="when-to-processing" tabindex="-1"><a class="header-anchor" href="#when-to-processing" aria-hidden="true">#</a> When To Processing</h3><p><code>Structured data or semi-structured data</code>: An IoT device that captures temperature and sends that as a number to a system.</p><p><code>unstructured data</code>: data that collect audio or video files</p><ol><li>ETL</li><li>ELTL</li></ol><h3 id="kinesis" tabindex="-1"><a class="header-anchor" href="#kinesis" aria-hidden="true">#</a> Kinesis</h3><p>Kinesis: - data agnostic, you can seend data, that Json, XML, structured data or unstructured, to the same Kinesis Data Stream - data retention period and data consumption replay - pull-based mechanism</p><p>Kinesis Firehose:</p><pre><code>&gt; AWS realized that most customers were riding Kinesis consumers just to get data and move to S3 with minimal modification. It&#39;s just compression or encryption. To make your life easier, we created Amazon Kinesis Firehose.
</code></pre>`,13);function A(_,x){const t=l("ExternalLinkIcon");return o(),r("div",null,[d,e("p",null,[e("a",p,[a("kinesis"),n(t)]),a(" for real time ingestion.")]),e("p",null,[e("a",u,[a("Amazon API Gateway"),n(t)]),a(" for restful data ingestion.")]),h,e("ol",m,[g,e("li",null,[v,e("ul",null,[e("li",null,[e("p",null,[a("create policy, 参考"),e("a",f,[a("Policy generator"),n(t)])]),k]),b])]),S,y]),w])}const z=i(c,[["render",A],["__file","aws_data_lakes.html.vue"]]);export{z as default};
