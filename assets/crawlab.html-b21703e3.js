import{_ as a,Y as n,Z as r,a0 as l,a1 as i,a2 as e,a4 as c,E as t}from"./framework-957baa9a.js";const s={},d=l("h1",{id:"crawlab-试用",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#crawlab-试用","aria-hidden":"true"},"#"),i(" crawlab 试用")],-1),u={href:"https://www.crawlab.cn/why-crawlab/index.html",target:"_blank",rel:"noopener noreferrer"},_=l("h2",{id:"_1-调研爬虫管理框架-主要希望通过系统来解决如下问题",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#_1-调研爬虫管理框架-主要希望通过系统来解决如下问题","aria-hidden":"true"},"#"),i(" 1. 调研爬虫管理框架, 主要希望通过系统来解决如下问题")],-1),h=c("<li>爬虫管理 <ol><li>目前基本是谁开发, 谁部署, 谁管理</li><li>crawlab <ol><li>权限管理, 目前支持管理用户和普通用户(查看并操作自己的数据)</li><li>公共爬虫, 所有用户都可以进行看到, 但只有管理员可以修改爬虫</li></ol></li></ol></li><li>爬虫部署 <ol><li>目前部署, 使用github同步代码到特定服务器</li><li>crawlab部署 <ol><li>上传可以通过本地zip上传, 也可以在安装crawlab-sdk后, 命令行执行<code>crawlab upload</code></li><li>文件存储在主节点的MongoDB GridFS中, 主节点5s更新爬虫信息, 60s更新一次文件</li><li>主节点通过redis发布爬虫文件上传信息, 通知工作节点获取爬虫文件</li><li>工作节点获取到爬虫文件的消息后, 从MongoDB GridFS获取zip文件并解压到本地</li><li>持续部署 <ol><li>支持git管理, 可以从github, gitlab和gitee上拉取代码</li><li>可选手动和自动同步爬虫</li><li>Crawlab 的主节点（Master Node）将利用 git pull 操作将 Git 仓库的爬虫代码拉取（Pull）到主节点本地</li><li>主节点将拉取下来的爬虫代码通过 zip 打包的方式上传到 MongoDB GridFS</li><li>爬虫代码将被自动同步到所有在线节点</li></ol></li></ol></li><li>结果展示上, crawlab可以可视化查看所有项目已经各项目的运行状态</li></ol></li><li>爬虫调度 <ol><li>目前的做法, 使用命令行手动操作,配置crontab或依赖apscheduler启动进程来控制爬虫</li><li>crawlab <ol><li>通过可视化配置定时任务, 类crontab配置</li></ol></li></ol></li><li>日志采集 <ol><li>目前的做法, 日志保留在本地</li><li>crawlab <ol><li><code>捕获日志</code>通过Stdout标准输出流来捕获日志的，因此如果希望在 Crawlab 的界面中看到日志，就需要让日志内容输出到Stdout中。最简单的做法就是打印出来</li><li><code>异常检测</code>原理是通过正则表达式来完成的。默认会用 error、exception、traceback 来匹配日志内容判断该日志文本是否为错误日志</li><li><code>存储</code>日志和异常日志是分别储存在 MongoDB 数据库的 logs 和 error_logs collection 中的</li><li>支持不好, 可能<code>需要考虑其他监控方案</code></li></ol></li></ol></li><li>数据监控和写入 <ol><li>目前的做法, 通过爬虫汇报爬取结果数量, 结合监控系统来对爬取结果做监控</li><li>crawlab: <ol><li>Crawlab 默认是用 MongoDB 作为储存结果的数据库，而且要求结果数据必须与 Crawlab 运行数据库是同一个数据库</li><li><code>专业版</code> 做了更多数据源的集成, 目前支持了 MySQL、Postgres、ElasticSearch 数据库，以及 Kafka 消息队列</li></ol></li><li>考虑到目前大部分爬虫如kafka, 可以灵活安排,不必须集成: <ol><li>此处crawlab其实支持的也不好, 至少需要配置双写到mongo, 可能<code>需要考虑其他监控方案</code></li></ol></li></ol></li>",5),b=l("li",null,"目前的做法不存在子节点管理, 每个服务器都作为单独存在, 资源监控以及新的爬虫部署在哪台服务器, 依赖个人选择",-1),w=l("li",null,"添加子节点可以无需再登录这台设备部署和启动爬虫",-1),p=l("li",null,"添加子节点支持直接部署, docker和Kubernetes",-1),g=l("li",null,"通过给redis发送心跳消息来保持在线状态",-1),f=l("code",null,"专业版",-1),k={href:"https://docs.crawlab.cn/zh/Monitor/",target:"_blank",rel:"noopener noreferrer"},x=l("li",null,[i("环境管理 "),l("ol",null,[l("li",null,"目前每台电脑都是独立的, 可以随时配置修改依赖"),l("li",null,[i("crawlab的子节点安装依赖 "),l("ol",null,[l("li",null,[i("自动安装依赖, 爬虫根目录读取"),l("code",null,"requirements.txt")]),l("li",null,"通过可视化界面给子节点发送安装依赖命令"),l("li",null,"可以添加脚本, 在配置节点时完成依赖安装")])])])],-1),m=l("li",null,[i("灵活性担忧 "),l("ol",null,[l("li",null,"目前crawlab多节点启动任务, 只支持随机/指定节点/全部节点这三种, 意味着如果需要2-5个节点同时运行, 是不是就得创建同样多的任务"),l("li",null,[i("如果被识别为scrapy爬虫, 执行命令默认"),l("code",null,"scrapy crawl"),i(", 显示中如果有需要同步执行一些顺序操作, 之前会写在一个"),l("code",null,".py"),i("文件中执行, 现在可能需要考虑其他方案")])])],-1),B=l("h1",{id:"问题",tabindex:"-1"},[l("a",{class:"header-anchor",href:"#问题","aria-hidden":"true"},"#"),i(" 问题")],-1),M=l("ol",null,[l("li",null,[i("随机的逻辑如何选择 "),l("ol",null,[l("li",null,"看代码")])]),l("li",null,"对性能的整合"),l("li",null,[i("灵活度 "),l("ol",null,[l("li",null,"如果在spider的close里整合下一个流程, 影响灵活性"),l("li",null,"spider之间会有关联, 不是完全解藕")])])],-1);function S(C,D){const o=t("ExternalLinkIcon");return n(),r("div",null,[d,l("p",null,[i("Crawlab 是一个分布式爬虫管理平台, 主要解决的是大量爬虫管理困难的问题, "),l("a",u,[i("参考"),e(o)])]),_,l("ol",null,[h,l("li",null,[i("服务器管理 "),l("ol",null,[b,l("li",null,[i("crawlab子节点管理 "),l("ol",null,[w,p,g,l("li",null,[f,i("可以对MongoDB 数据库、Redis 数据库、主节点、工作节点的性能数据进行"),l("a",k,[i("监控"),e(o)])])])]),x])]),m]),B,M])}const E=a(s,[["render",S],["__file","crawlab.html.vue"]]);export{E as default};
