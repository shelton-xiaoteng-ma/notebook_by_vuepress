import{_ as o,Y as i,Z as l,a0 as s,a1 as n,a2 as a,a4 as t,E as r}from"./framework-957baa9a.js";const c={},p=s("h1",{id:"firehose-调研",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#firehose-调研","aria-hidden":"true"},"#"),n(" firehose 调研")],-1),u={href:"https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html",target:"_blank",rel:"noopener noreferrer"},d=s("code",null,"kinesis firehose",-1),h=s("code",null,"kinesis data stream",-1),m={href:"https://www.whizlabs.com/blog/aws-kinesis-data-streams-vs-aws-kinesis-data-firehose/",target:"_blank",rel:"noopener noreferrer"},_=s("ol",null,[s("li",null,"Data Streams 有更低延迟和大规模读取数据的能力. Firehose 更注重数据存储."),s("li",null,"Data Streams 梳理更实时, 延迟在200ms 或 70ms, 而 Firehose 依赖于缓冲配置, 至少60s"),s("li",null,"回溯能力, Data Streams 支持最长7天的数据回放, Firehose 则不支持, 需要从存放后的数据中读取"),s("li",null,"扩展方面, Firehose 全自动, Data Streams 可以手动配置"),s("li",null,"数据写入方面, KDS 和 KDF 是类似的链接, KDS 兼容 SDK, 其他数据源方面基本一样"),s("li",null,"数据消费方面, KDS 对消费者开放式支持, 可以支持多个消费端使用, 同时支持 Spark 和 KCL, 而 KDF 是封闭式, Firehose 管理消费端, 也不支持 Spark 和 KCL")],-1),k=s("li",null,"总的来说, firehose 更注重数据存储, 如果希望分析流数据, 同时数据存储, 则在 data stream 基础上接入 firehose 就合理",-1),b=s("li",null,[n("Kinesis Data Streams "),s("img",{src:"https://docs.aws.amazon.com/images/streams/latest/dev/images/architecture.png",alt:"Kinesis Data Streams"})],-1),f=s("li",null,[n("Kinesis Data Firehose "),s("img",{src:"https://docs.aws.amazon.com/images/firehose/latest/dev/images/fh-flow-s3.png",alt:"Data Flow"})],-1),v=s("h2",{id:"数据写入",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#数据写入","aria-hidden":"true"},"#"),n(" 数据写入")],-1),g={href:"https://docs.aws.amazon.com/firehose/latest/dev/basic-write.html",target:"_blank",rel:"noopener noreferrer"},y={href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/firehose.html",target:"_blank",rel:"noopener noreferrer"},K={href:"https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/firehose/client/put_record_batch.html",target:"_blank",rel:"noopener noreferrer"},D=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># Each PutRecordBatch request supports up to 500 records. Each record in the request can be as large as 1,000 KB (before base64 encoding), up to a limit of 4 MB for the entire request. These limits cannot be changed.</span>
<span class="token keyword">def</span> <span class="token function">send_records_to_firehose</span><span class="token punctuation">(</span>firehose_client<span class="token punctuation">,</span> firehose_stream<span class="token punctuation">,</span> records<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 最多500条, 单条1000k, 总请求&lt;4M</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    response = client.put_record_batch(
        DeliveryStreamName=&#39;string&#39;,
        Records=[
            {
                &#39;Data&#39;: b&#39;bytes&#39;
            },
        ]
    )
    &quot;&quot;&quot;</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>records<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        sub_records <span class="token operator">=</span> records<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span><span class="token number">500</span><span class="token punctuation">]</span>
        send_records <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">&#39;Data&#39;</span><span class="token punctuation">:</span> json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>record<span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token keyword">for</span> record <span class="token keyword">in</span> sub_records<span class="token punctuation">]</span>
        response <span class="token operator">=</span> firehose_client<span class="token punctuation">.</span>put_record_batch<span class="token punctuation">(</span>
            DeliveryStreamName<span class="token operator">=</span>firehose_stream<span class="token punctuation">,</span>
            Records<span class="token operator">=</span>send_records
        <span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&#39;FailedPutCount&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">&#39;RequestResponses&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&quot;sampleTempData.json&quot;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> json_file<span class="token punctuation">:</span>
    observations <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>json_file<span class="token punctuation">)</span>
client <span class="token operator">=</span> boto3<span class="token punctuation">.</span>Session<span class="token punctuation">(</span>region_name<span class="token operator">=</span><span class="token string">&#39;cn-northwest-1&#39;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>client<span class="token punctuation">(</span><span class="token string">&#39;firehose&#39;</span><span class="token punctuation">)</span>
stream_name <span class="token operator">=</span> <span class="token string">&#39;PUT-S3-U29xq&#39;</span>
send_records_to_firehose<span class="token punctuation">(</span>client<span class="token punctuation">,</span> stream_name<span class="token punctuation">,</span> observations<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),w={href:"https://catalog.us-east-1.prod.workshops.aws/workshops/32e6bc9a-5c03-416d-be7c-4d29f40e55c4/en-US/lab-1/lab1-1-direct-put",target:"_blank",rel:"noopener noreferrer"},q={href:"https://github.com/jamesabrannan/kinesis_client_tutorial.git",target:"_blank",rel:"noopener noreferrer"},F=t("<li>可能存在的问题 <ol><li>json序列化问题 (datetime)</li><li>特殊字符或异常内容, 非json会破坏后续数据结构, 正常结构:<code>{&quot;station&quot;: &quot;GA&quot;, &quot;temp&quot;: &quot;1049.64F&quot;}{&quot;station&quot;: &quot;NY&quot;, &quot;temp&quot;: &quot;1027.70F&quot;}</code></li><li>数据过大</li><li>高并发写入, 1000/s 的并发写入, 至少要支持100次/每个实例/秒 <ul><li>参考海外区描述: When Direct PUT is configured as the data source, each Kinesis Data Firehose delivery stream provides the following combined quota for PutRecord and PutRecordBatch requests: 500,000 records/second, 2,000 requests/second, and 5 MiB/second. 100,000 records/second, 1,000 requests/second, and 1 MiB/second.</li><li>问：为什么我在向 Amazon Kinesis Data Firehose 传输流发送数据时会受到限制？ 默认情况下，每个传输流每秒最多可以接收 2000 个事务、5000 条记录和 5MB 数据。您可以通过提交提高服务限制表来轻松地提高此限制。</li><li>实测</li></ul></li></ol></li>",1),S=s("h2",{id:"数据输出",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#数据输出","aria-hidden":"true"},"#"),n(" 数据输出")],-1),B=t("<li><p>路径:</p><ul><li>默认路径: <code>s3://test-bigonelab/2023/03/14/07/PUT-S3-U29xq-1-2023-03-14-07-12-09-f9200397-76c2-4aad-a7ea-986ae76f5a64.gz</code>, 此时配置: <code>s3://test-bigonelab</code>, 前缀为空</li><li>若设置为: <code>s3://test-bigonelab/bolib/bolib_to_bologstats/year=2023/month=03/day=15/</code><ul><li>配置前缀: <code>bolib/bolib_to_bologstats/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}</code></li><li>错误前缀: <code>bolib/bolib_to_bologstats/!{firehose:error-output-type}/!{timestamp:yyyy/MM/dd}</code></li></ul></li></ul></li>",1),z=s("p",null,"动态分区和碎片化文件问题",-1),x=s("li",null,"不需要启用动态分区: 多记录解聚启用, 同时选择多记录解聚类型:JSON, 新换行分隔符不启用, JSON 内联解析不启用",-1),P={href:"https://docs.aws.amazon.com/firehose/latest/dev/s3-prefixes.html",target:"_blank",rel:"noopener noreferrer"},M=s("ul",null,[s("li",null,"firehose: !{firehose:error-output-type}, !{firehose:random-string}"),s("li",null,[n("timestamp: !{timestamp:yyyy}, !{timestamp:yyyy/MM/dd}..., 注意两点"),s("code",null,"The timestamp is always in UTC."),n(", "),s("code",null,"Kinesis Data Firehose uses the approximate arrival timestamp of the oldest record")]),s("li",null,"partitionKeyFromLambda"),s("li",null,"partitionKeyFromQuery")],-1),A=s("li",null,"这里使用timestamp就可以将数据按天分区",-1),R=s("li",null,[n("尽管打算固定输出前缀, 后面按天分区, 但是开启动态分区要求添加分区键, 暂时添加"),s("code",null,"kafka_topic"),s("ul",null,[s("li",null,[s("code",null,"bolib/!{partitionKeyFromQuery:kafka_topic}/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/")])])],-1),C=s("li",null,[s("p",null,"约束数据格式, 方便后续读取便利, 和之前的kafka数据保持一致"),s("ul",null,[s("li",null,"如果不开启动态分区, 所有记录会写在一行, 还需要研究下怎么同步格式")])],-1),j=s("h2",{id:"配置实例",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#配置实例","aria-hidden":"true"},"#"),n(" 配置实例")],-1),N=s("h3",{id:"topic-firehose实例对应关系",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#topic-firehose实例对应关系","aria-hidden":"true"},"#"),n(" topic/firehose实例对应关系")],-1),T=s("p",null,"每个topic创建一个流, 直接写入S3",-1),V=s("li",null,"单个实例的资源上限, 写入S3的资源上限, 可能会影响性能",-1),E={href:"https://docs.amazonaws.cn/firehose/latest/dev/limits.html",target:"_blank",rel:"noopener noreferrer"},U=t('<h3 id="不需要启用数据转换" tabindex="-1"><a class="header-anchor" href="#不需要启用数据转换" aria-hidden="true">#</a> 不需要启用数据转换</h3><ol><li>Amazon Lambda 转换 ?</li><li>json to Parquet or ORC ? <ul><li>Parquet 和 ORC 是列式数据格式，与 JSON 等行式格式相比，前者可节省空间并更快地启用查询。</li></ul></li></ol><h3 id="不需要动态分区" tabindex="-1"><a class="header-anchor" href="#不需要动态分区" aria-hidden="true">#</a> 不需要动态分区</h3><h3 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h3><ol><li>缓冲区: 128MiB / 900秒, 一旦达到指定缓冲提示的值之一，就会触发记录传输。 <ul><li>问：如果我选择压缩数据的话，缓冲区大小怎样计算？ 缓冲区大小在压缩前应用。因此，如果您选择压缩数据，Amazon S3 存储桶中的对象大小可能小于您指定的缓冲区大小。</li></ul></li><li>压缩: GZIP, 不加密</li></ol><h3 id="异常覆盖" tabindex="-1"><a class="header-anchor" href="#异常覆盖" aria-hidden="true">#</a> 异常覆盖</h3><ol><li><p>写入异常:</p><ul><li>Each PutRecordBatch request supports up to 500 records. Each record in the request can be as large as 1,000 KB (before base64 encoding), up to a limit of 4 MB for the entire request. These limits cannot be changed.</li><li>失败后的重试: <ul><li>在bolib读取返回内容, 选择性重试</li><li>失败数据写入某处做标记</li></ul></li></ul></li><li><p>写入重复:</p><ul><li>Kinesis Data Firehose 使用 at-least-once 语义进行数据交付。在某些情况下，例如当数据传输超时时，如果原始数据传输请求最终通过，Kinesis Data Firehose 重试传输可能会引入重复内容。这适用于 Kinesis Data Firehose 支持的所有目的地类型。</li></ul></li><li><p>消费异常:</p><ul><li>到 S3 存储桶的数据传输可能会由于某些原因而失败。 <ul><li>例如，存储桶可能已不存在，Kinesis Data Firehose 担任的 IAM 角色可能无法访问存储桶、网络故障或类似事件。在这种情况下，Kinesis Data Firehose 会持续重试长达 24 小时，直到交付成功。Kinesis Data Firehose 的最长数据存储时间为 24 小时。如果数据传输失败超过 24 小时，数据将丢失。</li><li>问：如果向 Amazon S3 存储桶传输数据失败，会发生什么情况？ <ul><li>如果向 Amazon S3 存储桶传输数据失败，Amazon Kinesis Data Firehose 最长可在 24 小时内每 5 秒重新传输一次数据。如果该问题在超过 24 小时的最长保留期仍然存在，Amazon Kinesis Data Firehose 将丢弃这些数据。</li></ul></li></ul></li><li>如果需要数据解析, 设置解析异常s3路径</li><li>对于无法解析的数据, 如果保存</li></ul></li><li><p>数据备份是否需要</p></li></ol><h2 id="成本估算" tabindex="-1"><a class="header-anchor" href="#成本估算" aria-hidden="true">#</a> 成本估算</h2>',8),L={href:"https://www.amazonaws.cn/kinesis/data-firehose/pricing/",target:"_blank",rel:"noopener noreferrer"},G=s("li",null,"Kinesis Data KDF 传输流的基本功能是摄取和传输。摄取采取分层定价，按照 5KB 的增量以每 GB 计费（3KB 记录按 5KB 计费，12KB 记录按 15KB 计费，等等）。除非使用可选功能，否则不会收取额外的 Kinesis Data KDF 费用。",-1),I={href:"https://docs.aws.amazon.com/firehose/latest/dev/limits.html",target:"_blank",rel:"noopener noreferrer"},O={href:"https://www.amazonaws.cn/kinesis/data-firehose/faqs/",target:"_blank",rel:"noopener noreferrer"},J=s("ul",null,[s("li",null,"向上取整计算 5KB 发生在记录级别而非 API 操作级别。例如，如果您的 PutRecordBatch 调用包含两条 1KB 的记录，那么该调用的数据量将被计为 10KB。（每条记录 5KB）")],-1),Q=s("li",null,"转换格式, 暂时不需要开启",-1),W=s("li",null,[n("VPC传输, 判断是否涉及 "),s("ul",null,[s("li",null,"对于目标在 Amazon VPC 中的传输流，您需要根据该传输流在每个可用区保持活跃状态的时间按小时付费。未满一小时将按一小时计费。按照摄取的每 GB 数据，收取数据处理费用。")])],-1),Y=s("li",null,"S3动态分区费用, 暂时不需要开启",-1),Z=t('<h2 id="讨论" tabindex="-1"><a class="header-anchor" href="#讨论" aria-hidden="true">#</a> 讨论</h2><ol><li><code>如果按照一个topic一条数据流实例的形式, 这不需要json内联解析</code></li><li>数据碎片问题: 128M/15分钟最长, 128MiB也是压缩前的数据量, s3落盘文件依旧会比较碎</li><li>方案和费用关联: <ul><li>s3碎片</li><li>VPC传输</li><li>单条记录碎片问题(向上取整5kb)</li></ul></li><li>大量写入能力的测试: <ul><li>读取数据</li><li>测试异常</li></ul></li></ol>',2);function H(X,$){const e=r("ExternalLinkIcon");return i(),l("div",null,[p,s("p",null,[n("为何选择 firehose, 既然可以直接写入"),s("a",u,[d,a(e)]),n(" , 为什么还需要 "),h,n(" ?")]),s("ul",null,[s("li",null,[n("参考"),s("a",m,[n("Kinesis Data Streams Vs AWS Kinesis Data Firehose"),a(e)]),_]),k,b,f]),v,s("ol",null,[s("li",null,[n("写入方式一些参考 "),s("ul",null,[s("li",null,[s("p",null,[n("官方文档 "),s("a",g,[n("Sending Data to an Amazon Kinesis Data Firehose Delivery Stream"),a(e)])])]),s("li",null,[s("p",null,[s("a",y,[n("boto3 firehose client"),a(e)]),n(" / "),s("a",K,[n("put_record_batch"),a(e)])]),D]),s("li",null,[s("p",null,[n("workshops 教程, "),s("a",w,[n("Direct Put as data source"),a(e)])])]),s("li",null,[s("p",null,[s("a",q,[n("python kinesis_client_tutorial"),a(e)])])])])]),F]),S,s("ol",null,[B,s("li",null,[z,s("ul",null,[x,s("li",null,[s("a",P,[n("动态分区前缀"),a(e)]),n(", 有4种namespace "),M]),A,R])]),C]),j,N,T,s("ul",null,[V,s("li",null,[s("a",E,[n("配额限制, 默认情况下，每个账户在每个区域最多可以有 50 个 Kinesis Data Firehose 传输流"),a(e)])])]),U,s("blockquote",null,[s("p",null,[n("没有设置费用，也无需预先承诺。Kinesis Data Firehose 有四种类型的按需使用：摄取、格式转换、VPC 传输和动态分区。可能会收取额外的数据传输费用。"),s("a",L,[n("费用参考"),a(e)])])]),s("ol",null,[s("li",null,[n("摄入，每 GB "),s("ol",null,[G,s("li",null,[s("a",I,[n("Quota 说按记录计费"),a(e)])]),s("li",null,[s("a",O,[n("定价和账单"),a(e)]),n(" 问：当我使用 PutRecordBatch 操作向 Amazon Kinesis Data Firehose 发送数据时，如何向上取整计算 5KB？ "),J])])]),Q,W,Y]),Z])}const ns=o(c,[["render",H],["__file","firehose_research.html.vue"]]);export{ns as default};
